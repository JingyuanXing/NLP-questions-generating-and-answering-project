# -*- coding: utf-8 -*-
"""Untitled.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wJsgNwgOq34RctIR6AQoqiq7lbKBDigv
"""

from google.colab import drive
drive.mount('/content/gdrive')

import nltk
from nltk.tokenize import word_tokenize
from nltk.tokenize import sent_tokenize
#nltk.download("popular")

r = open(file = "a1.txt", mode = "r")
text = r.read()

text = text.replace("(", ", ")
text = text.replace(")", ", ")
text_lines = text.splitlines()

sent_tokens = sent_tokenize(text_lines[4])
sents_tagged = [nltk.pos_tag(word_tokenize(sent)) for sent in sent_tokens]
sents_tagged

# {((<NNP*>|<NNPS*>)<POS>)<JJ>*<NN>}
#     {<DT>(<NN>|<NNS>)*<POS>?<JJ>*<NN>}
#     {(<PRP\$>|<DT>)<JJ>*<NN>}
#     {<JJ>*<NN>+}
#     {<NN>|<NNS>|<NNP>|<NNPS>}
   
# PP: {<IN><NP>}
#    {<TO><NP>}

grammar = r"""

NP: {<PRP\$|DT>?<JJ.*>*(<NN.*>+<CC>?<NN.*>*)(<POS>?<IN>?<TO>?<DT>?<JJ>*<NN.*>+)+(<,>(<POS>?<IN>?<TO>?<DT>?<JJ>*<NN.*>+)+(<,>)?)}    
    {<PRP\$|DT>?<JJ.*>*(<NN.*>+<CC>?<NN.*>*)(<,>(<POS>?<IN>?<TO>?<DT>?<JJ>*<NN.*>+)+(<,>)?)+} 
    {<PRP\$|DT>?<JJ.*>*(<NN.*>+<CC>?<NN.*>*)(<IN>?<TO>?<POS>?<DT>?<JJ>*(<NN.*>+<CC>?<JJ.*>*<NN.*>*)+)*}
    {<NN.*>+}
    {<PRP\$><NN>}
    {<PRP>}
    {<NP>+<IN><TO><NP>+}
      
VP: #{<NP>?<PRP>?(<VB>|<VBZ>|<VBP>|<VBN>|<VBD>)+<NP>*<PP>*}

    {<RB>*<VB.*><PRP\$>?<NP|CLAUSE>}
    {<VBD|VBZ><JJ.*>*<RB.*>*<VBN><TO>?<IN>?<NP|CLAUSE>(<CC><IN|TO><NP>)*}
    
CLAUSE: {<NP><VP><PP>?}


AD: {<RB>*<VBN|VBD>?<IN><PRP\$>*<NP>}
    {<IN><TO><NP>}


CLAUSE: {<WDT|WP|WP$|WRB|CC>?<AD>*<NP><VP><AD>*}
        {<AD>*<NP><AD>*<VP><AD>*}
        {<NP><VP>}

        
        
"""
cp = nltk.RegexpParser(grammar, loop=1)
result = cp.parse(sents_tagged[0])
print(result)

import re
import string

def NP_Comma_Process(wordslst):
    w2sent = " ".join(wordslst)
    w2_parts = w2sent.partition(",")
    print(w2_parts)
    if len(w2_parts) == 1:
        return None
    elif len(w2_parts)> 5:
        return None
    else:
        print(w2_parts[0])
        return [(w2_parts[0],),tuple(["is"]),(w2_parts[2],)]

relations = []
from nltk import Tree
import string
if_main_clause = True
for i,child in enumerate(result):
    if isinstance(child, Tree) and child.label()=="CLAUSE":
        NP = list()
        Actions = list()
        NP2 = list()
        for j, gchild in enumerate(child):
            if not isinstance(gchild,Tree) and gchild[1] in ['WDT','WP','WP$','WRB','CC'] and j == 0:
                if_main_clause = False
            elif isinstance(gchild, Tree) and gchild.label() == "NP":
                words, tags = zip(*gchild.leaves())
                NP.append(" ".join(words))
                
            elif isinstance(gchild, Tree) and gchild.label() == "VP":
                for k, ggchild in enumerate(gchild):
                    if not isinstance(ggchild, Tree) and ggchild[1] in ['VBZ', "VBD", "VBN", "IN" , "TO"]:
                        Actions.append(ggchild[0])
                    elif isinstance(ggchild, Tree) and ggchild.label() in ['CLAUSE', "NP"]:
                        words, tags = zip(*ggchild)
                        NP2.append(" ".join(words))
                        comma_results = NP_Comma_Process(words)
                        if comma_results is not None:
                            relations.append(comma_results)
        a = [tuple(NP), tuple(Actions), tuple(NP2)]
        relations.append(a)

print(relations)

"""**Kristin**: **the plain question generating** from the relation extracted, continued from the above section of code. It uses Standford NER to recognize the terms and replace them with question words. Stanford NER package is not good enough in recognizing every term, but we have the option to train it with additional data, and I am working on that. We should also implement some regex matching, beside just letting Stanford NER to categorize the vocabs. 

**Directions for running Stanford NER on your computer:**

Actually you only need to download the folder from https://nlp.stanford.edu/software/CRF-NER.shtml#Download  with all the Stanford NER materials and put it in the relative folder as described in the path in this following piece of code. No installation step needed. 

Output from the following piece of code, given the result of relation extraction on the material "a1.txt", is:

`['What is the period in the third millennium , c. 2686-2181 BC , ?', 'The Old Kingdom is what ?', 'What includes the great 4th Dynasty ?', 'it includes what ?', 'it includes the great what organization Dynasty ?', 'it includes the great 4th what organization ?', 'What perfected the art of pyramid building and the pyramids of Giza ?', 'King Sneferu perfected what ?', 'King who perfected the art of pyramid building and the pyramids of Giza ?', 'King Sneferu perfected the art of pyramid building and the pyramids of where ?']`
"""

jar = 'stanford-ner/stanford-ner.jar'
# model = 'stanford-ner/classifiers/english.all.3class.distsim.crf.ser.gz'
model = 'stanford-ner/classifiers/english.muc.7class.distsim.crf.ser.gz'
# model = 'stanford-ner/classifiers/wikigold.conll.ser.gz'
# model = 'stanford-ner/classifiers/sentiment.ser.gz'
st = StanfordNERTagger(model, jar, encoding='utf-8')

# Qestion Generation 1
# Substitute type questions

questions = []
for r in relations:
    # print(r)
    subj = r[0][0]
    obj = r[2][0]
    tokenized_subj = word_tokenize(subj)
    classified_subj = st.tag(tokenized_subj)
    print(classified_subj)
    tokenized_obj = word_tokenize(obj)
    classified_obj = st.tag(tokenized_obj)
    print(classified_obj)

    # compile the words in the subjects and objects into phrases.
    phrase = "".join(["".join([x+" " for x in w]) for w in r])

    # general question with 'what' replacement. 
    phrase_no_subj = "".join(["".join([x+" " for x in w]) for w in [r[1], r[2]]])
    phrase_no_obj = "".join(["".join([x+" " for x in w]) for w in [r[0], r[1]]])
    questions.append("What " + phrase_no_subj + "?")
    questions.append(phrase_no_obj + "what " + "?")

    # specific questions targeting the named entity
    for word_tuple in classified_subj + classified_obj:
        if (word_tuple[1] == 'PERSON'):
            questions.append(re.sub(r'\b%s\b' % word_tuple[0], "who", phrase, 1) + "?")
        if (word_tuple[1] == 'LOCATION'):
            questions.append(re.sub(r'\b%s\b' % word_tuple[0], "where", phrase, 1) + "?")   
        if (word_tuple[1] == 'ORGANIZATION'):
            questions.append(re.sub(r'\b%s\b' % word_tuple[0], "what organization", phrase, 1) + "?")   


print(questions)